1. For the given dataset, apply the suitable data preprocessing techniques ( any one) and
visualize the details of original as well as preprocessed dataset with the help of visualization
techniques ( min.5)

2. For the given dataset, apply all the preprocessing techniques and display the results of
preprocessed data.

6. Apply data cleaning preprocessing techniques for the given dataset , apply two visualization
techniques on it and generate frequent itemset using suitable algorithm.


==>
import numpy as np
import pandas as pd

df = pd.read_csv('train.csv')



Preprocessing techniques:
1) Data Reduction
specify column names which you want

subset = df[['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'Cabin']]
subset.head()

2) Data Cleaning
handling missing values
df.isnull().sum()		->  if it contains value other than 0 then

	(1) if it is integer type
		df.Age.fillna(df['Age'], inplace=True)
	(2) if it is object
		df['Cabin'].fillna(str(df['Cabin'].mode().values[0]), inplace=True)


3) Data Inntegration
// remove any 1 column
subset2=df[['PassengerId', 'Survived', 'Sex', 'Age', 'Cabin']]
subset2.head()

subset3=pd.merge(subset,subset2,on='Survived')
subset3.head()



4) Data Transformation
dummies=pd.get_dummies(df['Sex'])
dummies

// remove column
df=df.drop(['Sex'],1)
df

// add that column to df3 and view it
df3=pd.concat((df,dummies),1)
df3





VISUALIZATION TECHNIQUES:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

1) 
sns.countplot(data=df,x='Col_name1')
sns.countplot(data=df,x='Col_name1',hue='Col_name2')

2) 
ax=df.plot(figsize=(18,12), title='House Info')

3)
df['Col_name'].plot(kind='bar')
df.plot.bar()

4)
df['Col_name'].plot(kind='hist')
df.plot.hist()

5)
df.plot.scatter(x='Col_name1',y='Col_name2')

6)
df.plot.hexbin(x='Col_name1',y='Col_name2', gridsize=5)










10. For the given dataset, preprocess it and then display the details through visualization
techniques for the original as well as preprocessed dataset( max.10 visualization techniques
to be covered) 

=>

apply visualization techniques to original data

preprocess data using above methods (like handling missing values)
df.Age.fillna(df['Age'], inplace=True)

again apply visualization techniques for this data












3. Implement K means clustering algorithm for the suitable assumed dataset by your own
9. Implement K means clustering algorithm to show the cluster formation. 

=>

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.cluster import KMeans

df = pd.read_csv("driver's data.csv")

df.head()

df.isnull().sum()

# Defining x and y value
X = df.iloc[:, [2, 3]].values

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)

kmeans = KMeans(n_clusters = 3)
y_kmeans = kmeans.fit_predict(X)

plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
plt.title('Clusters of Drivers')
plt.xlabel('Concentration Score (Score1)')
plt.ylabel('Response Score (Score2)')
plt.legend()
plt.show()













5. Implement naÃ¯ve Bayes classifier and calculate accuracy for the suitable data.

=>

import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB 

df = pd.read_csv('diabetes.csv')

df.head()


df.isnull().sum()

x = df.drop(["Col_name"], axis=1)
y = df["Col_name"]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=2)


NB = GaussianNB()


NB.fit(x_train,y_train)

y_pred = NB.predict(x_test)

from sklearn import metrics
metrics.accuracy_score(y_test, y_pred)












7. Implement similarity or dissimilarity measures for any suitable data that you assumed.
8. Implement cosine similarity measure, mikownski distance measures for the suitable data
and show the results.

=>

import pandas as pd

df = pd.read_csv('IRIS.csv')

1) Euclidean distance 
from math import*
 
def euclidean_distance(x,y):
    return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))
print(euclidean_distance([0,3,4,5],[7,6,3,-1]))




2) Manhattan distance 
from math import*
 
def manhattan_distance(x,y):
    return sum(abs(a-b) for a,b in zip(x,y))
print(manhattan_distance([10,20,10],[10,20,20]))




3) Minkowski distance 
from math import*
from decimal import Decimal
 
def nth_root(value, n_root):
    root_value = 1/float(n_root)
    return round (Decimal(value) ** Decimal(root_value),3)
 
def minkowski_distance(x,y,p_value):
    return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),p_value)
print (minkowski_distance([0,3,4,5],[7,6,3,-1],3))




4) Cosine Similarity
from math import*
 
def square_rooted(x):
    return round(sqrt(sum([a*a for a in x])),3)
 
def cosine_similarity(x,y):
    numerator = sum(a*b for a,b in zip(x,y))
    denominator = square_rooted(x)*square_rooted(y)
    return round(numerator/float(denominator),3)
print(cosine_similarity([3, 45, 7, 2], [2, 54, 13, 15]))














4. Apply the Apriori algorithm for the given dataset or your own suitable data and show the
generation of association rules for the same.


=>


import numpy as pi
import matplotlib.pyplot as plt
import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules


data=pd.read_csv("CanteenDataSet.csv")

// rename column
data.columns=['Dishes']

from mlxtend.preprocessing import TransactionEncoder 


transactions=[]
items=data['Dishes'].values
print(items)



for i in range (0,len(items)):
    transactions.append(items[i].split(","))
    
print(transactions)



encoder=TransactionEncoder()
tran=encoder.fit(transactions).transform(transactions)
print(tran)



encodedData=pd.DataFrame(data = tran, columns = encoder.columns_, dtype = int)
print(encodedData)


encodedData.head(4)




support=apriori(encodedData,min_support=0.2,use_colnames=True)
support.sort_values(by = 'support', ascending = False)
confidence = association_rules(support, metric = 'confidence', min_threshold = 0.3)
confidence.sort_values(by = 'confidence', ascending = False)